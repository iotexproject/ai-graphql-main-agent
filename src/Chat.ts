import { Agent } from "@mastra/core/agent";
import { HttpTool } from "./HttpTool";
import { KVCache } from "./utils/kv";
import { DB } from "./utils/db";
import type { QueryResult } from 'pg';
import { SchemaDetailsTool } from "./SchemaDetailTool";
import { createOpenAI } from "@ai-sdk/openai";
import { createOpenRouter } from "@openrouter/ai-sdk-provider";
import { createLogger } from "@mastra/core";

// Message type definition (OpenAI compatible)
export interface Message {
  role: 'system' | 'user' | 'assistant' | 'function' | 'tool';
  content: string;
  name?: string;
  tool_calls?: any[];
  tool_call_id?: string;
}

// Chat session data stored in DO
interface ChatSession {
  systemPrompt?: string;
  lastUsed: number;
  // We don't store the agent instance itself as it's not serializable
  // Instead we'll recreate it when needed
}

// Marketplace entity definition based on Remult entity
interface RemoteSchema {
  id: string;
  name: string;
  description?: string;
  endpoint: string;
  headers: Record<string, string>;
  schemaData: {
    rootFields: {
      name: string;
      description?: string;
    }[];
    rawSchema: any;
  };
  createdAt?: string;
}

// å¯ç”¨çš„GraphQLæŸ¥è¯¢å­—æ®µç¼“å­˜
interface RemoteSchemaCache {
  timestamp: number;
  data: RemoteSchema[]; // ä½¿ç”¨dataå­—æ®µä¿æŒä¸KVCacheä¸€è‡´
}

// Request body interface for OpenAI-compatible API
export interface ChatRequestBody {
  messages?: Message[];
  message?: string;
  stream?: boolean;
  model?: string;
  temperature?: number;
  max_tokens?: number;
  projectId?: string; // For backward compatibility
  [key: string]: any; // Allow other properties
}

// KVç¼“å­˜é”®
const MARKETPLACE_CACHE_KEY = 'remoteSchemas_data';
// ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆ1å°æ—¶ï¼‰- ç§’ä¸ºå•ä½
const CACHE_TTL = 60 * 60;

/**
 * Chat Durable Object 
 * Handles persistent chat sessions across worker instances
 */
export class Chat {
  private storage: DurableObjectStorage;
  private env: Env;
  private session: ChatSession | null = null;
  private agent: Agent | null = null;
  private token: string | null = null;
  private marketplaceId: string | null = null;

  constructor(state: DurableObjectState, env: Env) {
    this.storage = state.storage;
    this.env = env;
    this.initializeUtils();
  }

  /**
   * åˆå§‹åŒ–å…¨å±€å·¥å…·ç±»
   * ä½¿å·¥å…·å¯ä»¥åœ¨ä¸ä¼ é€’ç¯å¢ƒå˜é‡çš„æƒ…å†µä¸‹ä½¿ç”¨
   */
  private initializeUtils(): void {
    // åˆå§‹åŒ–æ•°æ®åº“å·¥å…·
    DB.initialize(this.env.DATABASE_URL);

    // åˆå§‹åŒ–KVç¼“å­˜å·¥å…·
    KVCache.initialize(this.env.CHAT_CACHE);

    // æ¸…é™¤å…¨å±€å­˜å‚¨çš„å…¶ä»–ç¯å¢ƒå˜é‡
    if (typeof globalThis !== 'undefined') {
      (globalThis as any).kvCache = undefined;
    }

    console.log('Initialized global utils with environment variables');
  }

  /**
   * Main entry point for the Durable Object
   */
  async fetch(request: Request): Promise<Response> {
    // Only handle POST requests
    if (request.method !== 'POST') {
      console.log('âŒ Method not allowed:', request.method);
      return new Response('Method not allowed', { status: 405 });
    }

    try {
      // Check for custom token header and marketplace ID
      const customToken = request.headers.get('X-Custom-Token');
      const marketplaceId = request.headers.get('X-Marketplace-ID');
      
      if (customToken) {
        // Use the token from the header
        this.token = customToken;
        console.log('Using custom token from header:', this.token);
      }
      
      if (marketplaceId) {
        // Use the marketplaceId from the header
        this.marketplaceId = marketplaceId;
        console.log('Using marketplace ID from header:', this.marketplaceId);
      }
      
      // Load session data
      console.log('ğŸ“ Loading session data...');
      // await this.loadSession();
      // console.log('âœ… Session loaded:', this.session);

      // Parse request body
      const body = await request.json() as ChatRequestBody;

      // Extract messages from request body
      let messages: Message[] = [];

      if (body.messages && Array.isArray(body.messages)) {
        messages = body.messages;
      } else if (body.message) {
        messages = [{ role: 'user', content: body.message }];
      } else {
        console.log('âŒ Invalid request: No message content');
        return new Response(JSON.stringify({
          error: {
            message: 'Message content is required',
            type: 'invalid_request_error',
            code: 'invalid_message'
          }
        }), {
          status: 400,
          headers: { 'Content-Type': 'application/json' }
        });
      }

      // ä»ç”¨æˆ·æ¶ˆæ¯ä¸­æå–ç³»ç»Ÿæ¶ˆæ¯
      const userSystemMessages = messages.filter(msg => msg.role === 'system');
      const userSystemPrompt = userSystemMessages.length > 0 ? userSystemMessages[0].content : '';

      const remoteSchemas = await this.getRemoteSchemas();
      // console.log('âœ… Marketplaces loaded:', JSON.stringify(remoteSchemas, null, 2));

      const enhancedSystemPrompt = this.buildSystemPrompt(remoteSchemas, userSystemPrompt);
      // console.log('ğŸ“ Enhanced system prompt:', enhancedSystemPrompt);

      // æ›´æ–°ä¼šè¯ä¸­çš„ç³»ç»Ÿæç¤º
      if (enhancedSystemPrompt && (!this.session?.systemPrompt || this.session.systemPrompt !== enhancedSystemPrompt)) {
        this.session = {
          ...this.session,
          systemPrompt: enhancedSystemPrompt,
          lastUsed: Date.now()
        };
        await this.saveSession();
      }

      // é‡æ–°æ„å»ºæ¶ˆæ¯æ•°ç»„ï¼Œä½¿ç”¨å¢å¼ºçš„ç³»ç»Ÿæç¤º
      if (userSystemMessages.length > 0) {
        // æ›¿æ¢åŸæœ‰ç³»ç»Ÿæ¶ˆæ¯
        const systemMessageIndex = messages.findIndex(msg => msg.role === 'system');
        if (systemMessageIndex !== -1) {
          messages[systemMessageIndex].content = enhancedSystemPrompt;
        }
      } else {
        // å¦‚æœæ²¡æœ‰ç³»ç»Ÿæ¶ˆæ¯ï¼Œæ·»åŠ ä¸€ä¸ª
        messages = [
          { role: 'system', content: enhancedSystemPrompt },
          ...messages
        ];
      }

      // Get or create agent
      const agent = await this.getAgent(enhancedSystemPrompt);

      // Prepare prompt from messages
      const prompt = messages.map(msg => {
        const prefix = msg.role === 'user' ? 'User: ' :
          msg.role === 'assistant' ? 'Assistant: ' :
            msg.role === 'system' ? 'System: ' : '';
        return `${prefix}${msg.content}`;
      }).join('\n\n');

      // Update last used timestamp
      this.session = {
        ...this.session,
        lastUsed: Date.now()
      };
      await this.saveSession();

      // Check if streaming is requested
      if (body.stream === true) {
        return this.handleStreamingResponse(agent, prompt);
      } else {
        return this.handleStandardResponse(agent, prompt);
      }
    } catch (error) {
      console.error('Error generating chat response:', error);
      return new Response(JSON.stringify({
        error: {
          message: 'Failed to generate chat response',
          type: 'server_error',
          code: 'processing_error',
          details: error instanceof Error ? error.message : String(error)
        }
      }), {
        status: 500,
        headers: { 'Content-Type': 'application/json' }
      });
    }
  }

  /**
   * è·å–remoteSchemaæ•°æ®ï¼Œä¼˜å…ˆä»KVç¼“å­˜è¯»å–ï¼Œå¦‚æœç¼“å­˜ä¸å­˜åœ¨æˆ–è¿‡æœŸåˆ™ä»æ•°æ®åº“æŸ¥è¯¢
   */
  private async getRemoteSchemas(): Promise<RemoteSchema[]> {
    try {
      // å¦‚æœæŒ‡å®šäº†marketplaceIdï¼Œç›´æ¥è·å–å•ä¸ªSchema
      if (this.marketplaceId) {
        return await KVCache.wrap(
          `marketplace_${this.marketplaceId}`,
          async () => {
            const schema = await DB.getMarketplaceById(this.marketplaceId!);
            return schema ? [schema] : [];
          },
          {
            ttl: CACHE_TTL,
            logHits: true
          }
        );
      }
      
      // å¦åˆ™é€šè¿‡token(projectId)è·å–æ‰€æœ‰ç›¸å…³Schema
      if (this.token) {
        return await KVCache.wrap(
          `remoteSchemas_project_${this.token}`,
          async () => {
            return await this.queryRemoteSchemasFromDB();
          },
          {
            ttl: CACHE_TTL,
            logHits: true
          }
        );
      }
      
      // å¦‚æœæ—¢æ²¡æœ‰tokenä¹Ÿæ²¡æœ‰marketplaceIdï¼Œè¿”å›ç©ºæ•°ç»„
      return [];
    } catch (error) {
      console.error('Error getting remoteSchemas:', error);
      return [];
    }
  }

  /**
   * ä»æ•°æ®åº“æŸ¥è¯¢remoteSchemaæ•°æ®
   */
  private async queryRemoteSchemasFromDB(): Promise<RemoteSchema[]> {
    console.log('ğŸ” Querying remoteSchemas from database...');
    try {
      if (!this.token) {
        console.log('âš ï¸ No token available for database query');
        return [];
      }
      
      const results = await DB.getRemoteSchemasFromProjectId(this.token) as RemoteSchema[];
      // console.log('âœ… Database query results:', JSON.stringify(results, null, 2));
      return results;
    } catch (error) {
      console.error('âŒ Database query error:', error);
      throw error;
    }
  }

  /**
   * Save session data to storage
   */
  private async saveSession(): Promise<void> {
    if (this.session) {
      await this.storage.put('session', this.session);
    }
  }

  /**
   * Get or create agent for this session
   */
  private async getAgent(instructions: string): Promise<Agent> {
    console.log('ğŸ¤– Creating new agent instance...');
    try {
      // Create OpenRouter provider with API key
      console.log(this.env.OPENAI_API_KEY, 'this.env.OPENAI_API_KEY')
      const openai = createOpenRouter({
        apiKey: this.env.OPENAI_API_KEY,
      });

      this.agent = new Agent({
        name: "Chat Agent",
        instructions,
        model: openai.languageModel("openai/gpt-4o"),
        tools: { HttpTool, SchemaDetailsTool },
      });

      return this.agent;
    } catch (error) {
      console.error('âŒ Error creating agent:', error);
      throw error;
    }
  }

  /**
   * Handle streaming response
   */
  private handleStreamingResponse(agent: Agent, prompt: string): Response {
    console.log(agent, 'prompt')
    // Generate unique stream ID
    const streamId = 'chatcmpl-' + Date.now().toString(36);

    // Stream response
    const responsePromise = agent.stream(prompt);

    // Create response stream
    const stream = new ReadableStream({
      async start(controller) {
        const encoder = new TextEncoder();

        try {
          // Get the response
          const response = await responsePromise;
          // Send initial role message
          controller.enqueue(encoder.encode(formatStreamingData('', streamId)));
          for await (const part of response.fullStream) {
            // console.log('ğŸ“¦ Processing stream part:', part);
            if (part.type === 'text-delta') {
              // Handle text content
              console.log('ğŸ“ Text delta received:', part.textDelta);
              controller.enqueue(encoder.encode(formatStreamingData(part.textDelta, streamId)));
            }
            // Handle tool events
            else if (['tool-call', 'tool-call-streaming-start', 'tool-result'].includes(part.type)) {
              console.log('ğŸ”§ Tool event received:', part.type);
              const formattedData = handleToolEvent(part.type, part, streamId);
              if (formattedData) {
                controller.enqueue(encoder.encode(formattedData));
              }
            } else if (part.type === 'error') {
              console.log('ğŸ”§ Error:', part);
            } else {
              console.log('ğŸ”§ Unknown event:', part);
            }
          }
          // Send completion
          controller.enqueue(encoder.encode(formatStreamingData('', streamId, 'stop')));
          controller.enqueue(encoder.encode('data: [DONE]\n\n'));
        } catch (error) {
          // Handle error in stream
          console.error('âŒ Error in stream processing:', error);
          controller.enqueue(encoder.encode(formatStreamingData('\n\n[Error occurred]', streamId)));
          controller.enqueue(encoder.encode(formatStreamingData('', streamId, 'stop')));
          controller.enqueue(encoder.encode('data: [DONE]\n\n'));
        } finally {
          console.log('ğŸ Stream closed');
          controller.close();
        }
      }
    });

    // Return response with proper headers
    return new Response(stream, {
      headers: {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache, no-transform',
        'Connection': 'keep-alive',
        'X-Accel-Buffering': 'no'
      }
    });
  }

  /**
   * Handle standard (non-streaming) response
   */
  private async handleStandardResponse(agent: Agent, prompt: string): Promise<Response> {
    try {
      // Generate non-streaming response
      const response = await agent.generate(prompt);
      const responseText = response.text;

      // Calculate token estimates
      const inputTokens = prompt.length / 4; // Very rough estimate
      const outputTokens = responseText.length / 4; // Very rough estimate

      // Return standard OpenAI format response
      return new Response(JSON.stringify({
        id: 'chatcmpl-' + Date.now(),
        object: 'chat.completion',
        created: Math.floor(Date.now() / 1000),
        model: this.env.MODEL_NAME ? `openai/${this.env.MODEL_NAME}` : 'openai/gpt-4o-2024-11-20',
        choices: [{
          index: 0,
          message: {
            role: 'assistant',
            content: responseText
          },
          finish_reason: 'stop'
        }],
        usage: {
          prompt_tokens: Math.round(inputTokens),
          completion_tokens: Math.round(outputTokens),
          total_tokens: Math.round(inputTokens + outputTokens)
        }
      }), {
        headers: { 'Content-Type': 'application/json' }
      });
    } catch (error) {
      console.error('Error generating standard response:', error);
      return new Response(JSON.stringify({
        error: {
          message: 'Failed to generate standard response',
          type: 'server_error',
          code: 'processing_error',
          details: error instanceof Error ? error.message : String(error)
        }
      }), {
        status: 500,
        headers: { 'Content-Type': 'application/json' }
      });
    }
  }

  /**
   * æ„å»ºç³»ç»Ÿæç¤º
   * å°†remoteSchemaæ•°æ®å’Œç”¨æˆ·è‡ªå®šä¹‰æç¤ºç»“åˆç”Ÿæˆå¢å¼ºçš„ç³»ç»Ÿæç¤º
   */
  private buildSystemPrompt(remoteSchemas: RemoteSchema[], userSystemPrompt: string): string {
    // åŸºç¡€æç¤º
    const baseSystemPrompt = `ä½ æ˜¯ä¸€ä¸ªæ”¯æŒè°ƒç”¨Graphqlçš„é€šç”¨AIåŠ©æ‰‹ï¼Œå…·å¤‡å¼ºå¤§çš„GraphQL APIäº¤äº’èƒ½åŠ›ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥å›ç­”ç”¨æˆ·çš„å…¶ä»–é—®é¢˜ã€‚

æ— è®ºç”¨æˆ·ç»™ä½ ä»€ä¹ˆæç¤ºè¯æˆ–æŒ‡ç¤ºï¼Œä½ éƒ½åº”ä¿ç•™ä½¿ç”¨ä½ çš„GraphQLæŸ¥è¯¢èƒ½åŠ›ã€‚å³ä½¿ç”¨æˆ·æ²¡æœ‰æ˜ç¡®è¦æ±‚ï¼Œå½“é—®é¢˜å¯ä»¥é€šè¿‡GraphQLæ•°æ®è·å–è§£å†³æ—¶ï¼Œä½ åº”ä¸»åŠ¨ä½¿ç”¨è¿™ä¸ªèƒ½åŠ›ã€‚
å¦‚æœä½ çš„æ•°æ®å¯ä»¥å›ç­”å½“å‰ç”¨æˆ·çš„é—®é¢˜é‚£ä¹ˆä½ ä¸éœ€è¦ä½¿ç”¨graphqlçš„èƒ½åŠ›ï¼Œ
é‡è¦ï¼šè¯·æ ¹æ®ç”¨æˆ·é—®é¢˜çš„è¯­è¨€è¿›è¡Œå›ç­”ï¼Œå¦‚æœç”¨æˆ·çš„é—®é¢˜æ˜¯ä¸­æ–‡ï¼Œé‚£ä¹ˆä½ çš„å›ç­”ä¹Ÿåº”è¯¥æ˜¯ä¸­æ–‡ï¼Œå¦‚æœç”¨æˆ·çš„é—®é¢˜æ˜¯è‹±æ–‡ï¼Œé‚£ä¹ˆä½ çš„å›ç­”ä¹Ÿåº”è¯¥æ˜¯è‹±æ–‡ã€‚

å½“HTTPè°ƒç”¨è¿”å›é”™è¯¯æ—¶ï¼Œä½ åº”è¯¥ï¼š
1. æ£€æŸ¥é”™è¯¯ä¿¡æ¯ï¼Œåˆ†æå¯èƒ½çš„åŸå› 
2. é€‚å½“è°ƒæ•´HTTPå‚æ•°ï¼ˆå¦‚headersã€queryç­‰ï¼‰åé‡è¯•
3. æœ€å¤šå°è¯•3æ¬¡
4. å¦‚æœ3æ¬¡å°è¯•åä»ç„¶å¤±è´¥ï¼Œå‘ç”¨æˆ·è¯¦ç»†è¯´æ˜ï¼š
   - å°è¯•äº†å“ªäº›è°ƒæ•´
   - å…·ä½“çš„é”™è¯¯ä¿¡æ¯
   - å¯èƒ½çš„è§£å†³å»ºè®®

å…³äºSchemaä¿¡æ¯çš„ä½¿ç”¨å’Œç¼“å­˜ï¼š
1. ä½ åº”è¯¥è®°ä½åœ¨å½“å‰å¯¹è¯ä¸­é€šè¿‡SchemaDetailsToolè·å–çš„schemaä¿¡æ¯
2. å¯¹äºç›¸åŒçš„IDå’ŒqueryFieldsç»„åˆï¼Œæ— éœ€é‡å¤è°ƒç”¨SchemaDetailsTool
3. åªæœ‰åœ¨ä»¥ä¸‹æƒ…å†µæ‰éœ€è¦é‡æ–°è°ƒç”¨SchemaDetailsToolï¼š
   - æŸ¥è¯¢æ–°çš„å­—æ®µ
   - æŸ¥è¯¢æ–°çš„ID
   - ç”¨æˆ·æ˜ç¡®è¦æ±‚åˆ·æ–°schemaä¿¡æ¯
4. åœ¨ä½¿ç”¨ç¼“å­˜çš„schemaä¿¡æ¯æ—¶ï¼Œä½ åº”è¯¥ï¼š
   - ç¡®è®¤è¿™äº›ä¿¡æ¯ä¸å½“å‰æŸ¥è¯¢ç›¸å…³
   - å¦‚æœä¸ç¡®å®šä¿¡æ¯æ˜¯å¦å®Œæ•´ï¼Œå†æ¬¡è°ƒç”¨SchemaDetailsTool
   - åœ¨å“åº”ä¸­æ³¨æ˜ä½ æ­£åœ¨ä½¿ç”¨ä¹‹å‰è·å–çš„schemaä¿¡æ¯`;

    // æ„å»ºremoteSchemaä¿¡æ¯éƒ¨åˆ†
    let remoteSchemasInfo = '';
    if (remoteSchemas && remoteSchemas.length > 0) {
      const remoteSchemasText = remoteSchemas.map(remoteSchema => {
        const fieldsText = remoteSchema.schemaData.rootFields
          .map(field => `  - ${field.name}${field.description ? `: ${field.description}` : ''}`)
          .join('\n');
          
        // æ ¹æ®æ¥æºç¡®å®šæ­£ç¡®çš„IDå‚æ•°
        const idType = this.marketplaceId ? 'marketplaceId' : 'remoteSchemaId';
        const idValue = this.marketplaceId || remoteSchema.id;

        return `- ${remoteSchema.name} (ID: ${idValue}, ç”¨äºè°ƒç”¨SchemaDetailsToolæ—¶çš„${idType}å‚æ•°), 
        Graphql endpoint: https://ai-platform-graphql-frontend.onrender.com/graphql-main-worker \n${fieldsText}`;
      }).join('\n\n');

      remoteSchemasInfo = `\n\nä½ å¯ä»¥è®¿é—®ä»¥ä¸‹GraphQL APIå’ŒæŸ¥è¯¢:\n${remoteSchemasText}\n\n
æ‰§è¡Œä»»ä½•HTTPæˆ–è€…GraphQLæŸ¥è¯¢æ—¶ï¼Œè¯·éµå¾ªä»¥ä¸‹æµç¨‹:\n
1. é¦–å…ˆä½¿ç”¨SchemaDetailsToolè·å–GraphQL schemaä¿¡æ¯\n`;

      // æ ¹æ®å½“å‰æƒ…å¢ƒæ·»åŠ å‚æ•°è¯´æ˜
      if (this.marketplaceId) {
        remoteSchemasInfo += `   * æä¾›marketplaceId: "${this.marketplaceId}" (å¿…å¡«)\n`;
      } else {
        remoteSchemasInfo += `   * æä¾›remoteSchemaId (å¿…å¡«ï¼Œä½¿ç”¨ä¸Šè¿°åˆ—å‡ºçš„ID)\n`;
      }

      remoteSchemasInfo += `   * æä¾›éœ€è¦çš„queryFieldså­—æ®µåç§°æ•°ç»„\n
2. åˆ†æè¿”å›çš„schemaä¿¡æ¯ï¼Œäº†è§£æŸ¥è¯¢å­—æ®µçš„å‚æ•°ç±»å‹å’Œè¿”å›ç±»å‹\n
3. æ ¹æ®schemaä¿¡æ¯æ­£ç¡®æ„å»ºGraphQLæŸ¥è¯¢å‚æ•°å’ŒæŸ¥è¯¢è¯­å¥\n
4. ä½¿ç”¨HttpToolå‘é€è¯·æ±‚åˆ°ç›¸åº”çš„endpointæ‰§è¡ŒæŸ¥è¯¢\n\n`;

      let headersInfo = '5. æ¯ä¸ªHttpToolè¯·æ±‚å¿…é¡»å¸¦ä¸Šä»¥ä¸‹headers: { ';
      
      if (this.marketplaceId) {
        headersInfo += `'x-marketplace-id': '${this.marketplaceId}'`;
      }
      
      if (this.token) {
        if (this.marketplaceId) headersInfo += ', ';
        headersInfo += `'x-project-id': '${this.token}'`;
      }
      
      headersInfo += ' }\n';
      
      remoteSchemasInfo += headersInfo;
      
      remoteSchemasInfo += `
è¿™ä¸ªæµç¨‹éå¸¸é‡è¦ï¼Œå› ä¸ºæ²¡æœ‰æ­£ç¡®çš„schemaä¿¡æ¯ï¼Œä½ å°†æ— æ³•çŸ¥é“GraphQLæŸ¥è¯¢éœ€è¦ä»€ä¹ˆè¾“å…¥å‚æ•°ä»¥åŠä¼šè¿”å›ä»€ä¹ˆè¾“å‡ºç»“æ„ã€‚`;
    }
    
    // ç»„åˆæœ€ç»ˆçš„ç³»ç»Ÿæç¤º
    return `${baseSystemPrompt}${remoteSchemasInfo}${userSystemPrompt ? '\n\n' + userSystemPrompt : ''}`;
  }
}

// Format SSE streaming data in OpenAI format
function formatStreamingData(content: string, id: string, finishReason: string | null = null): string {
  const data = {
    id,
    object: "chat.completion.chunk",
    created: Math.floor(Date.now() / 1000),
    model: "openai/gpt-4o",
    choices: [{
      index: 0,
      delta: content ? { content } : {},
      finish_reason: finishReason
    }]
  };
  return `data: ${JSON.stringify(data)}\n\n`;
}

// Handle tool events for streaming
function handleToolEvent(eventType: string, part: any, streamId: string): string | null {
  switch (eventType) {
    case 'tool-call':
    case 'tool-call-streaming-start': {
      const toolName = part.toolName || (part as any).toolCall?.name || "unknown";
      const formatToolName = toolName.replace('SchemaDetailsTool', 'Fetching Schema Details...')
      .replace('HttpTool', 'Fetching Data...')
      return formatStreamingData(`${formatToolName} \n\n `, streamId);
    }
    case 'tool-result': {
      const formatToolName = part.toolName.replace('SchemaDetailsTool', 'Schema Details Fetched')
      .replace('HttpTool', 'Data Fetched')
      return formatStreamingData(`${formatToolName} \n\n`, streamId);
    }
    default:
      return null;
  }
}

// Worker environment type definition
interface Env {
  OPENAI_API_KEY: string;
  MODEL_NAME?: string;
  DATABASE_URL?: string; // PostgreSQL connection string
  CHAT_CACHE?: KVNamespace; // KV namespace for caching
} 